{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "196a54bd-14a2-4f79-8c5e-c0e2efcf1726",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **EDA for the target data - part 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9eb85ab-3dad-409e-bf81-edf525c776b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 0 - Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b82bf-5bf9-46e0-9d74-53682668c25d",
   "metadata": {},
   "source": [
    "### 0.1 - Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6758bd42-df4f-4fb4-bb6a-4fe74fa7119b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "sns.set() # set seaborn as default style\n",
    "\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "plt.rcParams.update({ \"figure.figsize\" : (8, 5),\"axes.facecolor\" : \"white\", \"axes.edgecolor\":  \"black\"})\n",
    "plt.rcParams[\"figure.facecolor\"]= \"w\"\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "#for dealing with outliers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "#for time\n",
    "import time\n",
    "\n",
    "\n",
    "#miscellania\n",
    "import scipy.stats as ss\n",
    "from IPython.display import display, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "RSEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060ae92-9d4d-4ec9-bdb3-d58ec417c58c",
   "metadata": {},
   "source": [
    "### 0.2 - Functions set up by the user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a524f969-ca9a-4ca5-8280-d8270ed1ea6b",
   "metadata": {},
   "source": [
    "#### Cramers correlation between two categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a040d-288a-4e93-afe9-9ad771a00b9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cramers_corrected_stat(df,cat_col1,cat_col2):\n",
    "    \"\"\"\n",
    "    This function spits out corrected Cramer's correlation statistic\n",
    "    between two categorical columns of a dataframe \n",
    "    \"\"\"\n",
    "    crosstab = pd.crosstab(df[cat_col1],df[cat_col2])\n",
    "    chi_sqr = ss.chi2_contingency(crosstab)[0]\n",
    "    n = crosstab.sum().sum()\n",
    "    r,k = crosstab.shape\n",
    "    phi_sqr_corr = max(0, chi_sqr/n - ((k-1)*(r-1))/(n-1))    \n",
    "    r_corr = r - ((r-1)**2)/(n-1)\n",
    "    k_corr = k - ((k-1)**2)/(n-1)\n",
    "    \n",
    "    result = np.sqrt(phi_sqr_corr / min( (k_corr-1), (r_corr-1)))\n",
    "    return round(result,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905eb15d-0c1a-4100-8b68-85f7023c458f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Anova p-value for test of no correlation between a numerical and a categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7da76-c4f0-4042-bdf9-6cd16b20540a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def anova_pvalue(df,cat_col,num_col):\n",
    "    \"\"\"\n",
    "    This function returns the anova p-value (probability of no correlation) \n",
    "    between a categorical column and a numerical column of a dataframe\n",
    "    \"\"\"\n",
    "    CategoryGroupLists = df.groupby(cat_col)[num_col].apply(list)\n",
    "    AnovaResults = ss.f_oneway(*CategoryGroupLists)\n",
    "    p_value = round(AnovaResults[1],6)\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2504af-f069-40d3-aa6d-3c006c10d325",
   "metadata": {},
   "source": [
    "### Isolation forest test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff0fc8-1f05-41b8-8691-11c2c4541132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def isolation_forest_test(series, contamination_rate = 0.1, random_state = 0, n_estimators = 100):\n",
    "    \"\"\"\n",
    "    Function that takes a time series and perform an isolation forest test\n",
    "    and returns an boolean array with True/False for is/ is not an outlier\n",
    "    \"\"\"\n",
    "    \n",
    "    #put the pandas series into the numpy array format\n",
    "    array = series.array.reshape(-1, 1)\n",
    "\n",
    "    # fit the isolation forest model\n",
    "    isolation_forest = IsolationForest(n_estimators = n_estimators, \n",
    "                                       contamination=contamination_rate, \n",
    "                                       random_state = random_state, n_jobs = -1)\n",
    "    isolation_forest.fit(array)\n",
    "\n",
    "    #predict outliers\n",
    "    is_outlier = isolation_forest.predict(array) == -1\n",
    "\n",
    "    return is_outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5686da-691f-4a8f-be08-3d60057ba009",
   "metadata": {},
   "source": [
    "#### Creating reports on the time series in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052ad61-8c7a-4b79-b16d-dadb3c80bc13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def time_series_reports(data, index, variable, series_id):\n",
    "    \"\"\"\n",
    "    Creates a dictionary wehere the keys are the series id and the values \n",
    "    are the corresponding reports on the series in a dataframe. The reports\n",
    "    contain start and en dates, number of points and missing values.\n",
    "    \"\"\"\n",
    "    \n",
    "    list_series_ids = sorted(list(set([series_id for series_id in data[series_id]])))\n",
    "    \n",
    "    series_reports_dict = {} #dict of series reports\n",
    "    \n",
    "    for id_ in list_series_ids: #loop through the series ids\n",
    "        #slice the datasets into time series\n",
    "        aux_series = data[data[series_id] == id_].set_index(index)[variable]\n",
    "\n",
    "        #retrieve info for the report on production\n",
    "        start_date = aux_series.index[0]\n",
    "        end_date = aux_series.index[-1]\n",
    "        n_points = aux_series.size\n",
    "        nans_series = aux_series[aux_series != aux_series]\n",
    "    \n",
    "        #make the dictionary of reports\n",
    "        series_reports_dict.update({id_ : [start_date, end_date, n_points, nans_series]})\n",
    "        \n",
    "    return series_reports_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5646a6-3b8c-416d-b2db-fa45211ebf20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rescale_time_series(series_reference, series_to_scale):\n",
    "    \"\"\"\n",
    "    Rescale the time series in a dictionary using as reference\n",
    "    its counterpart in another dictionary using the MinMax method.\n",
    "    \"\"\"\n",
    "\n",
    "    scaler = MinMaxScaler() #instanciated the MinMax scaler\n",
    "    scaler.fit(series_reference.array.reshape(-1,1))\n",
    "    aux_array = scaler.transform(series_to_scale.array.reshape(-1,1)).flatten()\n",
    "    \n",
    "    scaled_series = pd.Series(data = aux_array, \n",
    "                              index = series_to_scale.index)\n",
    "    \n",
    "    return scaled_series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5595d3b-f90d-4a54-8429-089ba1c545c8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 - Data documentation\n",
    "\n",
    "- `county` - An ID code for the county.\n",
    "- `is_business` - Boolean for whether or not the prosumer is a business.\n",
    "- `product_type` - ID code with the following mapping of codes to contract types: {0: \"Combined\", 1: \"Fixed\", 2: \"General service\", 3: \"Spot\"}.\n",
    "- `target` - The consumption or production amount for the relevant segment for the hour. The segments are defined by the county, is_business, and product_type.\n",
    "- `is_consumption` - Boolean for whether or not this row's target is consumption or production.\n",
    "- `datetime` - The Estonian time in EET (UTC+2) / EEST (UTC+3). It describes the start of the 1-hour period on which target is given.\n",
    "- `data_block_id` - All rows sharing the same data_block_id will be available at the same forecast time. This is a function of what information is available when forecasts are actually made, at 11 AM each morning. For example, if the forecast weather data_block_id for predictins made on October 31st is 100 then the historic weather data_block_id for October 31st will be 101 as the historic weather data is only actually available the next day.\n",
    "- `row_id` - A unique identifier for the row.\n",
    "- `prediction_unit_id` - A unique identifier for the county, is_business, and product_type combination. New prediction units can appear or disappear in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a910123-6c37-4620-b352-7df3626e77e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2 - Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72e802-1307-43ab-9254-70a93347c650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reading my csv file\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca853a-442e-452e-a13b-3d5fecb865e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7a5ae-a96b-4444-8468-c5f9a6146d84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#put date/time into the datetime type\n",
    "train_df['datetime'] = pd.to_datetime(train_df['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deac405d-3fe3-4f9f-9876-bb7d8977f882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#display info about the dataset and count non-NaNs values\n",
    "train_df.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e87f1-8a85-4ef9-9915-45883bf2a90a",
   "metadata": {},
   "source": [
    "The `target` columns has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a8e97-ef0f-48ff-8128-5613843f3ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#count the number of NaNs in target\n",
    "train_df['target'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5345b30-02ed-401b-9360-6b3399407318",
   "metadata": {},
   "source": [
    "Count how many timestamps per `prediction_unit_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b6658f-1440-4c0e-819d-b144b2873966",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.groupby('prediction_unit_id')['datetime'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2231ce-7379-4a47-8fec-2c52c9624001",
   "metadata": {},
   "source": [
    "Clearly some `prediction_unit_id` have either started been recorded later or stopped being recorded earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0204bfd-b3a8-4bc5-a74a-0110695f72aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3 - Getting unique `pediction_unit_id` values and make a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fd202-4305-48bd-a3b9-532c4ad69cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get unique predction_unit_id's\n",
    "pred_uni_ids = sorted(list(set([pred_id for pred_id in train_df['prediction_unit_id']])))\n",
    "aux_set = set(list(zip(train_df['county'], train_df['product_type'], train_df['is_business'], train_df['prediction_unit_id'])))\n",
    "aux_keys = []\n",
    "aux_values = []\n",
    "for elem in aux_set:\n",
    "    aux_keys.append(elem[3])\n",
    "    aux_values.append([elem[0], elem[1], elem[2]])\n",
    "\n",
    "pred_ids_dict = dict(sorted(dict(zip(aux_keys,aux_values)).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198ba19d-7d93-4c8b-b867-d302c5bf760c",
   "metadata": {},
   "source": [
    "We put that dictionary in the dataframe format and save as a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170896bd-974d-4302-8824-45c270303313",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_ids_df = pd.DataFrame(data = pred_ids_dict).transpose()\n",
    "pred_ids_df.reset_index(inplace = True)\n",
    "pred_ids_df.rename(columns = {'index' : 'prediction_unit_id', \n",
    "                              0 : 'county', \n",
    "                              1 : 'product_type', \n",
    "                              2 : 'is_business'}, \n",
    "                   inplace = True)\n",
    "pred_ids_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4e516e-84da-420b-b039-1e470e7a2ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_ids_df.to_csv('../data/prediction_unit_id_dictionary.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4a3b6a-8489-4044-b87c-8888ecaadcfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4 - EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807ce092-9e0d-41eb-bbf9-02e42618976e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.1 - Correlations\n",
    "\n",
    "We first separate the categorical from numerical features. Here only `target` is a numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea9ff60-fded-445a-8701-3acdf3e47dc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the specified features from the DataFrame\n",
    "cat_feats = ['county', 'is_business', \n",
    "             'product_type', 'is_consumption', \n",
    "             'prediction_unit_id']\n",
    "\n",
    "target = ['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49eec312-6674-434f-9fe1-d894cf38a6f3",
   "metadata": {},
   "source": [
    "#### Correlations amongst the categorical features\n",
    "\n",
    "To analyse the correlations amongst the *categorical features*, we employ __[*CramÃ©r's V correlation*](https://en.wikipedia.org/wiki/Cram%C3%A9r%27s_V)__ coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e4445-e94b-4d7a-bf5b-43344dd5f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dictionary of Cramer's correlation \n",
    "cramer_v_corr = dict(\n",
    "    zip(\n",
    "        cat_feats,\n",
    "        [[cramers_corrected_stat(train_df,f1,f2) for f2 in cat_feats] for f1 in cat_feats]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20957627-f280-4d4d-bf29-6da922f327ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cramer correlation heatmap\n",
    "\n",
    "plt.figure(figsize=(10,10),dpi=100)\n",
    "sns.heatmap(data=pd.DataFrame(data=cramer_v_corr,index=cat_feats),\n",
    "            cmap='magma',\n",
    "            linecolor='white',\n",
    "            linewidth=1,\n",
    "            annot=True,\n",
    "            vmin=-1,\n",
    "            vmax=1\n",
    "           );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d05b8c1-6b11-4a98-ad04-a059999802aa",
   "metadata": {},
   "source": [
    "We see that `is_business` is correlated with `product_type`. This makes sense. From __[Enefit's](https://www.energia.ee/en/ari/elekter/elektrileping-ja-paketid?consumers=large-consumer)__ website, we see that while large consumers (> 300MWh/year) have access to 4 different types of products, small consumers (< 300MWh/year) only have access to two. From the data documentation and the content in that webpage we can make up the following key: \n",
    "\n",
    "- (0) Combined -> Fixed and exchange price purchase solution\n",
    "- (1) Fixed -> Fixed price purchase solution\n",
    "- (2) General service -> Base energy and exchange price purchase solution\n",
    "- (3) Spot -> Exchange price purchase solution\n",
    "\n",
    "For large consumers all the 4 types are relevant, while for small consumers *(1) Fixed* and *(3) Spot* are relevant. This makes sense with the distribution of `product_type` for `is_consumption` = 0, 1 in the EDA for the `client` dataset.\n",
    "\n",
    "Moreover, we also see that `county` shows correlations with `is_business` and `product_type`. This makes sense, if we think in terms of demographics. Counties with large population, as Harju, Tartu and Ida-Viru, are more prone to have larger business and thus allow for product types more often thanin counties with smaller population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790947e-6f01-4ffa-8978-61963f32207a",
   "metadata": {},
   "source": [
    "#### Correlations amongst the categorical features and the target\n",
    "\n",
    "To analyse the correlations amongst the categorical features and target, we now employ the __[*ANOVA p-value*](https://en.wikipedia.org/wiki/Analysis_of_variance)__ method. This method computes the probablility of the categoral and numerical values to be correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbb3fbb-5c21-4d67-a89a-352202ac74cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a dictionary of anova p-value\n",
    "\n",
    "anova_pvalue_dict = dict(\n",
    "    zip(\n",
    "        cat_feats,\n",
    "        [[anova_pvalue(train_df.dropna(),f1,f2) for f2 in target] for f1 in cat_feats]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c7e408-b5ee-413c-bdf1-c4b3b03f72a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# heatmap of anova p-values\n",
    "\n",
    "plt.figure(figsize=(1,8),dpi=100)\n",
    "sns.heatmap(data=pd.DataFrame(data=anova_pvalue_dict,index=target).T,\n",
    "            cmap='viridis',\n",
    "            linecolor='white',\n",
    "            linewidth=1,\n",
    "            annot=True,\n",
    "            vmin=0,\n",
    "            vmax=1\n",
    "           );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db5bce9-9df9-4610-a7e7-1c18a5a54481",
   "metadata": {},
   "source": [
    "It seems that we cannot rule out correlations between the categorical features and the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8138154-fbfe-48b8-9990-a7299edb92e8",
   "metadata": {},
   "source": [
    "#### Production/consumption correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a767a51-dea4-4525-8847-142b70ce169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_df = train_df[train_df['is_consumption']==0].merge(train_df[train_df['is_consumption']==1], \n",
    "                                                       on = ['datetime','prediction_unit_id', \n",
    "                                                             'product_type', 'is_business', 'county'], \n",
    "                                                       how = 'inner', \n",
    "                                                      suffixes = ('_production', '_consumption'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f74ddb-a4a1-4164-81aa-371d91f95495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pearson correlation heatmap\n",
    "\n",
    "#plt.figure(figsize=(25,10),dpi=100)\n",
    "plt.title('Production-consumption correlation')\n",
    "sns.heatmap(data=aux_df[['target_production', 'target_consumption']].corr(method = 'pearson', \n",
    "                                         numeric_only = True), \n",
    "            cmap='coolwarm', linecolor='white', linewidth=1, \n",
    "            annot=True, vmin=-1, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b3ff07-6399-4c41-978d-09e1643ce386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(25,10),dpi=100)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,6), dpi=100)\n",
    "gs = gridspec.GridSpec(1,2)\n",
    "for i in [0,1]:\n",
    "    axes[i].set_title(f'Production-consumption correlations for is_business = {i}')\n",
    "    sns.heatmap(data=aux_df[aux_df['is_business'] == i][['target_production', 'target_consumption']].corr(method = 'pearson', \n",
    "                                         numeric_only = True), \n",
    "            cmap='coolwarm', linecolor='white', linewidth=1, \n",
    "            annot=True, vmin=-1, vmax=1, ax=axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c5602a-b3ef-4586-b803-7cf4775bedbe",
   "metadata": {},
   "source": [
    "It is interesting to notice that the Pearson correlation in the case of business is much higher than in the case for non-business. There can be different reasons for that, such as\n",
    "\n",
    "- energy independence and cost savings;\n",
    "- net metering and excess generation;\n",
    "- environmental initiatives and corporate social responsibility;\n",
    "- governament incentives and policies;\n",
    "- operational patterns and daylight hours;\n",
    "- scale of solar intallations;\n",
    "- economic trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32b4996-c5ea-46b7-a8f3-0bc04feaa353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#plt.figure(figsize=(25,10),dpi=100)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14,14), dpi=100)\n",
    "gs = gridspec.GridSpec(2,2)\n",
    "for i in [0,1,2,3]:\n",
    "    axes[int(i/2), int(i%2)].set_title(f'Production-consumption correlations for product_type = {i}')\n",
    "    sns.heatmap(data=aux_df[aux_df['product_type'] == i][['target_production', 'target_consumption']].corr(method = 'pearson', \n",
    "                                         numeric_only = True), \n",
    "            cmap='coolwarm', linecolor='white', linewidth=1, \n",
    "            annot=True, vmin=-1, vmax=1, ax=axes[int(i/2), int(i%2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3699e9c-0b26-4a47-bd68-b21ce9b8f01b",
   "metadata": {},
   "source": [
    "We see a higher, negative correlation for `product_type` = 0, 2, which are the product types also share by the small consumers. The reason for the negative correlation can be seasonality: low production and higher consumption during winter months and higher production and lower consumption during the summer months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e5c652-c282-456b-8704-c6e053d55412",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(25,10),dpi=100)\n",
    "fig, axes = plt.subplots(8, 2, figsize=(12,46), dpi=100)\n",
    "gs = gridspec.GridSpec(8,2)\n",
    "for i in range(16):\n",
    "    axes[int(i/2), int(i%2)].set_title(f'Production-consumption correlations for county = {i}')\n",
    "    sns.heatmap(data=aux_df[aux_df['county'] == i][['target_production', 'target_consumption']].corr(method = 'pearson', \n",
    "                                         numeric_only = True), \n",
    "            cmap='coolwarm', linecolor='white', linewidth=1, \n",
    "            annot=True, vmin=-1, vmax=1, ax=axes[int(i/2), int(i%2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde97f59-573f-4a78-b095-90a44d011dd2",
   "metadata": {},
   "source": [
    "When analysing the production-consumption correlation for each county, we notice that the higher correlation are also negative. This could be explain by seasonal effects. Higher correlations are found in counties with low population, thus probably made of many small consumer which would be more prone to seasonal effects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f727fd-3e18-4c54-81a5-f40e97e95bb4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.2 - `target` distribution\n",
    "\n",
    "We now take a look at the distribution of the `target`. We will first split the dataframe w.r.t. production and consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52df0d3-a9d3-4fe7-81a5-0c53b594277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['datetime', 'target', 'prediction_unit_id']\n",
    "\n",
    "pred_uni_ids = sorted(list(set([pred_id for pred_id in train_df['prediction_unit_id']])))\n",
    "\n",
    "cond_prod = train_df['is_consumption'] == 0\n",
    "cond_cons = train_df['is_consumption'] == 1\n",
    "\n",
    "prod_df = train_df[cond_prod][cols].copy()\n",
    "cons_df = train_df[cond_cons][cols].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b69c8-2d4b-4ae0-b735-061b8e07d228",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3), dpi = 100)\n",
    "sns.histplot(cons_df, x = 'target')\n",
    "plt.title('Distribution of target for consumption')\n",
    "plt.xlim(0,2000)\n",
    "plt.ylim(0,30000)\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d915eabc-802e-478b-b634-35daaa3d7398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3), dpi = 100)\n",
    "sns.histplot(prod_df['target'])\n",
    "plt.title('Distribution of target for production')\n",
    "plt.xlim(0,2000)\n",
    "plt.ylim(0,1500)\n",
    "plt.xlabel('target')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f5f67-e34e-436f-b77f-293ede99c58c",
   "metadata": {},
   "source": [
    "We now display the distribution of zero and non-zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5604bb-bea6-48d7-a59e-80b7f6a210d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_zeros_target = cons_df['target'] != 0.\n",
    "plt.figure(figsize=(8, 3), dpi = 100)\n",
    "sns.histplot(non_zeros_target)\n",
    "plt.title('Distribution of target for consumption')\n",
    "plt.xlabel('is_target_non_zero')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561545f-da08-4f37-adb5-a2be50da1387",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Fraction of non-zero values in consumption: {non_zeros_target.sum()/len(non_zeros_target)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cc153-a9d7-4b98-ba98-410f9cefe7f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_zeros_target = prod_df['target'] != 0.\n",
    "plt.figure(figsize=(8, 3), dpi = 100)\n",
    "sns.histplot(non_zeros_target)\n",
    "plt.title('Distribution of target for consumption')\n",
    "plt.xlabel('is_target_non_zero')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9065b-d4dc-49b1-bd08-036a3112c00d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Fraction of non-zero values in consumption: {non_zeros_target.sum()/len(non_zeros_target)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876de835-4bbf-4d15-b9f3-110ba0445ad1",
   "metadata": {},
   "source": [
    "The conclusion here is that the distribution for `target` is highly skewed. Moreover, in the case of production, there is an sizeble fraction of zeros in the sample. The skewness of the distributions is a problem for regressions models, which asume a normal distribution for the target variable.\n",
    "\n",
    "The distribution of the `target` values are skewed towards the small values, having a tail for large values. A way to bring large values closer is to use a logarithmic scale. In our case it is convenient to use the transformation $x \\to \\log (1 + x)$, which will bring larger values closer to small values but will keep zero fixed.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb444984-f2f0-4198-8af1-b0f58dd6c1ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3), dpi = 100)\n",
    "log1p_target = np.log1p(cons_df['target'])\n",
    "sns.histplot(log1p_target)\n",
    "plt.title('Distribution of target for consumption in log scale')\n",
    "plt.xlabel('log1p_target')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a885b7-841b-4cdd-9bc9-9776797302f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 3), dpi = 100)\n",
    "log1p_target = np.log1p(prod_df['target'])\n",
    "sns.histplot(log1p_target)\n",
    "plt.title('Distribution of target for production in log scale')\n",
    "plt.xlabel('log1p_target')\n",
    "plt.ylabel('frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df0687-ad4a-433e-a107-ce76938d0d03",
   "metadata": {
    "tags": []
   },
   "source": [
    "We see this idea works for the consumption, making its distribution more symmetrical, but not for production, due to the high occurance of zeros in the sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3772041-5751-4648-919e-b85e61ba1b41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for county in range(16):\n",
    "    sns.histplot(np.log1p(train_df[(train_df['is_business']==1)&(train_df['is_consumption']==1)&(train_df['county']==county)]['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5541b1e-c7d9-4f1a-86e2-d5856bed8ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for county in [0,2,5,7,11]:\n",
    "    sns.histplot(np.log1p(train_df[(train_df['is_business']==1)&(train_df['is_consumption']==1)&(train_df['county']==county)]['target'])).set_xlim(0,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2924c61-2d4a-4333-9faf-21037c2e9e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for county in [1,3,4,6,8,9,10,12,13,14,15]:\n",
    "    sns.histplot(np.log1p(train_df[(train_df['is_business']==1)&(train_df['is_consumption']==1)&(train_df['county']==county)]['target'])).set_xlim(0,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312df7db-9237-42f7-b166-fc75ca98adae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.histplot(np.log1p(train_df[(train_df['is_business']==1)&(train_df['is_consumption']==1)]['target'])).set_xlim(0,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4bc97-4881-4321-9905-6e3e0cacb48a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(25,10),dpi=100)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), dpi=100)\n",
    "gs = gridspec.GridSpec(2,2)\n",
    "for i in range(4):\n",
    "    axes[int(i/2), int(i%2)].set_title(f'Target consumption distribution for product_type = {i}')\n",
    "    aux_series = np.log1p(aux_df[(aux_df['is_business']==1)&(aux_df['product_type']==i)]['target_consumption'])\n",
    "    sns.histplot(aux_series, label = f'Contract {i}', kde = True, ax=axes[int(i/2), int(i%2)])\n",
    "    axes[int(i/2), int(i%2)].set_xlim(0,11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dbcedf-9e47-41b3-adb8-ca1f7ab3e7af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 2, figsize=(12,46), dpi=100)\n",
    "gs = gridspec.GridSpec(8,2)\n",
    "\n",
    "for i in range(16):\n",
    "    sns.histplot(np.log1p(aux_df[(aux_df['is_business']==1)&(aux_df['county']==i)]['target_consumption']), ax=axes[int(i/2), int(i%2)])\n",
    "#    axes[int(i/2), int(i%2)].set_xlim(0,11) \n",
    "#    axes[int(i/2), int(i%2)].set_ylim(0, 5000)\n",
    "    axes[int(i/2), int(i%2)].set_title(f'Target consumption distribution fort County {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626362c2-9212-402b-8433-408000452d6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.3 - Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73fc47d-7d85-4183-b471-d198a60a8b8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = train_df.duplicated().sum()\n",
    "print(duplicates)\n",
    "\n",
    "# Now, the data is cleaned from missing values and duplicates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6ccd95-fb95-46cd-8a03-03f56f73c554",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.4 - Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f308286-5479-4235-b99d-60735faf7695",
   "metadata": {},
   "source": [
    "In order to check for missing values in each individual time series, we will pivot the dataframes for production and consumption so eact column consists of an individual time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ae9ed-74a5-4a5a-bb1a-322de8c52b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod_time_series = prod_df.pivot(index='datetime', columns='prediction_unit_id', values='target')\n",
    "prod_time_series.rename_axis(None, inplace=True, axis=1)\n",
    "prod_time_series.rename(columns=lambda x: f\"unit_{x}\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc27975-3ce8-484c-9109-7ee146f65289",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cons_time_series = cons_df.pivot(index='datetime', columns='prediction_unit_id', values='target')\n",
    "cons_time_series.rename_axis(None, inplace=True, axis=1)\n",
    "cons_time_series.rename(columns=lambda x: f\"unit_{x}\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93848ff0-d668-4cef-9674-7082e5cb8073",
   "metadata": {},
   "source": [
    "Next, we creat a series of reports for the production and consumption series for each `production_unit_id`. The report is based on a dictionary containing the starting end ending date/time of the series, its length in term of number of points and a table with the NaNs and corresponding dates, if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515ae91c-bc31-41ab-a582-b5fdf668f95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_series_report = time_series_reports(data = cons_df, index = 'datetime', \n",
    "                                       variable = 'target', series_id = 'prediction_unit_id')\n",
    "prod_series_report = time_series_reports(data = prod_df, index = 'datetime', \n",
    "                                       variable = 'target', series_id = 'prediction_unit_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f917e-08ad-49b2-b8f3-da5cabaa659b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Report for consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76e2bd-80ee-4ef5-89f1-5d9a9fd04885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('REPORT FOR CONSUMPTION TIME SERIES')\n",
    "for id in pred_uni_ids:\n",
    "    elem = cons_series_report.get(id)\n",
    "    print(15*'-----')\n",
    "    print(f'Time series #{id}')\n",
    "    print('Starting date:', elem[0])\n",
    "    print('End date:', elem[1])\n",
    "    print('Length:', elem[2])\n",
    "    print('NaNs')\n",
    "    print(elem[3],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916c3a4-aa69-448c-9b14-f2497a59c228",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Report for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346b26a-bcfb-47cd-b475-7566e00454db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('REPORT FOR PRODUCTION TIME SERIES')\n",
    "for id in pred_uni_ids:\n",
    "    elem = prod_series_report.get(id)\n",
    "    print(15*'-----')\n",
    "    print(f'Time series #{id}')\n",
    "    print('Starting date:', elem[0])\n",
    "    print('End date:', elem[1])\n",
    "    print('Length:', elem[2])\n",
    "    print('NaNs')\n",
    "    print(elem[3],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6b7c65-2c79-457f-9d8f-69aef8b75395",
   "metadata": {},
   "source": [
    "From these reports we conclude that (1) the missing values corresponds to the start and the end of the summer time in 2021, 2022 and 2023 and that (2) the time series for production and consumption do not have the same length (or duration) in time, with some starting later or ending earlier than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3caeca-af79-499e-8d42-1e5525082785",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.5 - Inputing missing values due to summer time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9e4f8-a037-43cb-a042-7ebdcfec13b5",
   "metadata": {},
   "source": [
    "To impute the missing values we will keep the EET/EEST timezone convention and simply interpolate the time series with the mean of the nearest neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b44a3-fdbb-4ad1-9769-a12853343273",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(69):\n",
    "    prod_time_series[f'unit_{i}'] = 0.5*(prod_time_series[f'unit_{i}'].fillna(method = 'ffill', limit = 1) \n",
    "                                         + prod_time_series[f'unit_{i}'].fillna(method = 'bfill', limit = 1))\n",
    "    \n",
    "    cons_time_series[f'unit_{i}'] = 0.5*(cons_time_series[f'unit_{i}'].fillna(method = 'ffill', limit = 1) \n",
    "                                         + cons_time_series[f'unit_{i}'].fillna(method = 'bfill', limit = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d541d5-7d56-49f1-9db7-de7785dd27ed",
   "metadata": {},
   "source": [
    "We then update the `prod_df` and `cons_df` dataframes with the cleaned series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1b699-121e-4c2e-8251-254a06e0ca5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod_df = prod_time_series.melt(value_name = 'target', \n",
    "                                var_name = 'prediction_unit_id', \n",
    "                                ignore_index = False)\n",
    "prod_df.dropna(inplace = True)\n",
    "prod_df.reset_index(inplace = True)\n",
    "prod_df['prediction_unit_id'] = prod_df['prediction_unit_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fee7b-9748-4924-9ed6-b6af4c237ae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cons_df = cons_time_series.melt(value_name = 'target', \n",
    "                                var_name = 'prediction_unit_id', \n",
    "                                ignore_index = False)\n",
    "cons_df.dropna(inplace = True)\n",
    "cons_df.reset_index(inplace = True)\n",
    "cons_df['prediction_unit_id'] = cons_df['prediction_unit_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bdef69-417b-44f1-8f5a-fe36c3015059",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.6 - Plotting the individual consumption and production time series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec816ab5-7305-4fde-ba52-4b08ed0570e6",
   "metadata": {},
   "source": [
    "To plot the time series for each `prediction_unit_id` value it is convenient to first pivot the dataframe for both production and consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8687201-96d6-4e8a-a503-775ece7b6139",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(23, 3, figsize=(12,46), dpi=100)\n",
    "gs = gridspec.GridSpec(23,3)\n",
    "for i in range(69):\n",
    "    prod_time_series[f'unit_{i}'].plot(ax=axes[int(i/3),int(i%3)], use_index=False, label = 'cons', legend = True).set_title(f'unit_{i}')\n",
    "    cons_time_series[f'unit_{i}'].plot(ax=axes[int(i/3),int(i%3)], use_index=False, label = 'prod', legend = True)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f2bb66-af6b-47cf-84a2-e916390440b4",
   "metadata": {},
   "source": [
    "From the plots we see that we have outlier in some time series and some of them are only piecewise continuous within the dataset time window. The piecewise ones, such as `unit_68` might have to be treated separately later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7db8d-6429-415a-910a-c51fb34a5472",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.6 - Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b12eb0-0521-49a9-88c8-334c783734b5",
   "metadata": {},
   "source": [
    "#### Removing outliers for the individual time series\n",
    "\n",
    "In order to visualise the outliers, we will first rescale the time series with the MinMax method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ea0c2-1230-4902-b393-7920595e7cb6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(69):\n",
    "    scaler = MinMaxScaler()\n",
    "    prod_time_series[f'unit_{i}_scaled'] = scaler.fit_transform(prod_time_series[f'unit_{i}'].array.reshape(-1,1))\n",
    "    cons_time_series[f'unit_{i}_scaled'] = scaler.fit_transform(cons_time_series[f'unit_{i}'].array.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39488653-02b0-4d26-a722-1f92cd380a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaled_cols = [f'unit_{i}_scaled' for i in range(69)]\n",
    "prod_scaled_df = prod_time_series.melt(value_vars = scaled_cols,\n",
    "                                       value_name = 'target', \n",
    "                                       var_name = 'prediction_unit_id', \n",
    "                                       ignore_index = False)\n",
    "prod_scaled_df.reset_index(inplace = True)\n",
    "prod_scaled_df['prediction_unit_id'] = prod_scaled_df['prediction_unit_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1e178-2009-40fa-98b7-0b43d3cb0f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cons_scaled_df = prod_time_series.melt(value_vars = scaled_cols,\n",
    "                                       value_name = 'target', \n",
    "                                       var_name = 'prediction_unit_id', \n",
    "                                       ignore_index = False)\n",
    "cons_scaled_df.reset_index(inplace = True)\n",
    "cons_scaled_df['prediction_unit_id'] = cons_scaled_df['prediction_unit_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff55bfa-23c3-456a-a4fc-434a17d97338",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_time_series.drop(columns = scaled_cols, inplace = True)\n",
    "cons_time_series.drop(columns = scaled_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df693b-f82b-46b9-b24f-ed25060458a2",
   "metadata": {},
   "source": [
    "And finally make a box plot of all the 69 production time series.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf431d0-4c3d-447d-b80d-1d38375acc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10),dpi=100)\n",
    "plt.title('Box plot for the scaled production time series')\n",
    "sns.boxplot(data=prod_scaled_df, x = 'prediction_unit_id', y = 'target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f911ab71-488c-471f-9746-9720122fe4c3",
   "metadata": {},
   "source": [
    "We now repeat the above procedure for the consumption time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728fe8c4-19d4-402b-9a93-38b6f5f418d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10),dpi=100)\n",
    "plt.title('Box plot for the scaled consumption time series')\n",
    "sns.boxplot(data=cons_scaled_df, x = 'prediction_unit_id', y = 'target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7788aede-5b63-45d9-81f3-95f747e2df52",
   "metadata": {},
   "source": [
    "np.empty()From the above box plots we can clearly see that some of the time series have outliers, see e.g. time series with `prediction_unit_id` 31 and 61 and the gaps in the data distribution. \n",
    "\n",
    "Next, we will employ **isolation forest models** to detect the outliers. A relevant parameter is the `contamination_rate` that specifies the proportion of outliers in the dataset. We will choose it to be of order 1/(number of points) in the time series, as from the box plots above they are just a hand full. We first perform the test for the production time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67503ee4-56aa-481d-ad74-22418724a0a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create an empty dataframe\n",
    "is_outlier = np.array([], dtype = 'bool')\n",
    "\n",
    "#loop through the prediction_unit_id's\n",
    "for i in range(69):\n",
    "\n",
    "    #set the fraction of outliers\n",
    "    outlier_fraction = 6./prod_time_series[f'unit_{i}'].size\n",
    "    aux_series = prod_time_series[f'unit_{i}'].dropna()\n",
    "    #generate a boolean array with True/False for is/is not outlier\n",
    "    aux_array = isolation_forest_test(series = aux_series, \n",
    "                                      contamination_rate = outlier_fraction,\n",
    "                                      random_state = RSEED, \n",
    "                                      n_estimators = 150)\n",
    "    \n",
    "    is_outlier = np.append(is_outlier, aux_array)\n",
    "\n",
    "outliers_prod_df = prod_df.copy()\n",
    "outliers_prod_df['is_outlier'] = is_outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc3e21-55a3-4b73-a3fa-d380a4c57abf",
   "metadata": {},
   "source": [
    "And now for consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2011d66-baf8-4057-9b79-a209a2351961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create an empty dataframe\n",
    "is_outlier = np.array([], dtype = 'bool')\n",
    "\n",
    "#loop through the prediction_unit_id's\n",
    "for i in range(69):\n",
    "\n",
    "    #set the fraction of outliers\n",
    "    outlier_fraction = 7./cons_time_series[f'unit_{i}'].size\n",
    "    aux_series = cons_time_series[f'unit_{i}'].dropna()\n",
    "    #generate a boolean array with True/False for is/is not outlier\n",
    "    aux_array = isolation_forest_test(series = aux_series, \n",
    "                                      contamination_rate = outlier_fraction,\n",
    "                                      random_state = RSEED, \n",
    "                                      n_estimators = 150)\n",
    "    \n",
    "    is_outlier = np.append(is_outlier, aux_array)\n",
    "\n",
    "outliers_cons_df = cons_df.copy()\n",
    "outliers_cons_df['is_outlier'] = is_outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6dd5bd-a5bd-4e81-9e68-c2b0786e69ca",
   "metadata": {},
   "source": [
    "Here are the resulting datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e3688f-4e59-48b4-a0da-3be51af68d26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers_cons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b01ed7f-ae8c-4e31-b9d6-187ee35b1c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers_prod_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52246ab0-8550-4f77-a1e4-2dfc3a2eeaf5",
   "metadata": {},
   "source": [
    "We can now visually check that the detection test worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab70fa23-fd8d-4d52-91d6-d559554e83f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for pred_id in pred_uni_ids:\n",
    "#    pick_pred_id = outliers_prod_df['prediction_unit_id'] == pred_id\n",
    "#    plt.title(f'Production time series with prediction_unit_id = {pred_id}')\n",
    "#    ax = sns.scatterplot(data = outliers_prod_df[pick_pred_id], x = 'datetime', y = 'target', hue = 'is_outlier')\n",
    "#    display(ax)\n",
    "#    plt.show()\n",
    "#    time.sleep(2)\n",
    "#    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112c75f-1157-434c-9c1a-51ffeeac41a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for pred_id in pred_uni_ids:\n",
    "#    pick_pred_id = outliers_cons_df['prediction_unit_id'] == pred_id\n",
    "#    plt.title(f'Consumption time series with prediction_unit_id = {pred_id}')\n",
    "#    ax = sns.scatterplot(data = outliers_cons_df[pick_pred_id], x = 'datetime', y = 'target', hue = 'is_outlier')\n",
    "#    display(ax)\n",
    "#    plt.show()\n",
    "#    time.sleep(2)\n",
    "#    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb75d0-97f9-49ec-8945-27b0adf2b99c",
   "metadata": {},
   "source": [
    "We can now remove the outliers we have detected and then input them using the nearest neighbours mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea38c3-d32d-4782-b09d-6123cb4e9ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers_prod_df.loc[outliers_prod_df['is_outlier'], 'target'] = np.nan\n",
    "outliers_cons_df.loc[outliers_cons_df['is_outlier'], 'target'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22599d61-1643-49c9-aa9c-cfcc12b011a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "outliers_prod_df['target'] = 0.5*(outliers_prod_df['target'].fillna(method = 'bfill', limit = 1) \n",
    "                                  + outliers_prod_df['target'].fillna(method = 'ffill', limit = 1))\n",
    "outliers_cons_df['target'] = 0.5*(outliers_cons_df['target'].fillna(method = 'bfill', limit = 1) \n",
    "                                  + outliers_cons_df['target'].fillna(method = 'ffill', limit = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82fe0e8-c24a-4e87-a4b3-ca61abcfe77d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod_series_without_outliers = outliers_prod_df.pivot(index='datetime', \n",
    "                                                      columns='prediction_unit_id', \n",
    "                                                      values='target')\n",
    "prod_series_without_outliers.rename_axis(None, inplace=True, axis=1)\n",
    "prod_series_without_outliers.rename(columns=lambda x: f\"unit_{x}\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa64f5-70fd-4d3f-b92b-7044b5c8b7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cons_series_without_outliers = outliers_cons_df.pivot(index='datetime', \n",
    "                                                      columns='prediction_unit_id', \n",
    "                                                      values='target')\n",
    "cons_series_without_outliers.rename_axis(None, inplace=True, axis=1)\n",
    "cons_series_without_outliers.rename(columns=lambda x: f\"unit_{x}\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8ba2df-a8c5-439d-be32-62f0dffb52c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cons_series_without_outliers_scaled = cons_series_without_outliers.copy()\n",
    "prod_series_without_outliers_scaled = prod_series_without_outliers.copy()\n",
    "for i in range(69):\n",
    "    cons_series_without_outliers_scaled[f'unit_{i}_scaled'] = rescale_time_series(cons_time_series[f'unit_{i}'], \n",
    "                                                              cons_series_without_outliers[f'unit_{i}'])\n",
    "    prod_series_without_outliers_scaled[f'unit_{i}_scaled'] = rescale_time_series(prod_time_series[f'unit_{i}'], \n",
    "                                                              prod_series_without_outliers[f'unit_{i}'])\n",
    "\n",
    "unscaled_cols = [f'unit_{i}' for i in range(69)]\n",
    "cons_series_without_outliers_scaled.drop(columns = unscaled_cols, inplace = True)\n",
    "prod_series_without_outliers_scaled.drop(columns = unscaled_cols, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f073cab3-6a5e-4618-8e3d-534dcdb849e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cons_without_outliers_scaled_df = cons_series_without_outliers_scaled.melt(value_vars = scaled_cols,\n",
    "                                       value_name = 'target', \n",
    "                                       var_name = 'prediction_unit_id', \n",
    "                                       ignore_index = False)\n",
    "cons_without_outliers_scaled_df.reset_index(inplace = True)\n",
    "cons_without_outliers_scaled_df['prediction_unit_id'] = cons_without_outliers_scaled_df['prediction_unit_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f28162b-f871-4dbb-b8ab-28fa0a8d09b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod_without_outliers_scaled_df = prod_series_without_outliers_scaled.melt(value_vars = scaled_cols,\n",
    "                                       value_name = 'target', \n",
    "                                       var_name = 'prediction_unit_id', \n",
    "                                       ignore_index = False)\n",
    "prod_without_outliers_scaled_df.reset_index(inplace = True)\n",
    "prod_without_outliers_scaled_df['prediction_unit_id'] = prod_without_outliers_scaled_df['prediction_unit_id'].apply(lambda x: int(x.split('_')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a2ce7-8247-4ad9-bd97-8bc1929ef5c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10),dpi=100)\n",
    "plt.title('Box plot for the scaled production time series')\n",
    "sns.boxplot(data=prod_without_outliers_scaled_df, x = 'prediction_unit_id', y = 'target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01650ef8-9902-481d-8a6f-5035634540ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10),dpi=100)\n",
    "plt.title('Box plot for the scaled consumption time series')\n",
    "sns.boxplot(data=cons_without_outliers_scaled_df, x = 'prediction_unit_id', y = 'target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37051e6a-5518-4f7c-9e82-ec1811714bda",
   "metadata": {},
   "source": [
    "**NOTE:** even though we have eliminated the outliers in the individual time series at this point in the notebook, we have not saved them in the final file produced here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373cf5b-ffd2-495f-9c00-f2eb3ef44488",
   "metadata": {},
   "source": [
    "#### No outliers for the whole `target` column\n",
    "\n",
    "To see that there are no outlier when we stack all the production or the consumption time series together, let us make scatter plot of `target` against `datetime` for all the `prediction_unit_id`s.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd98a3-e592-4096-90ae-6705af7d80de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10),dpi=100)\n",
    "sns.scatterplot(data = prod_df, x = 'datetime', y = 'target')\n",
    "plt.title('Production target data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b571d59-6c19-4c17-8546-190cd9bcaa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10),dpi=100)\n",
    "sns.scatterplot(data = cons_df, x = 'datetime', y = 'target')\n",
    "plt.title('Consumption target data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12849e0-d1f1-48a8-80ff-d7dea0b27380",
   "metadata": {},
   "source": [
    "From the scatter plots we do not really see outliers for the stacked production and consumption data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a647a245-2853-48c0-880b-0cf751d60644",
   "metadata": {},
   "source": [
    "### 4.8 - Updating the dataframe and saving the cleaned file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2c5c33-3c64-4154-9e9c-97da25567326",
   "metadata": {},
   "source": [
    "Now that we have gotten rid of the **missing values** in the time series, we can plug them back into our dataframe `train_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0286b8e8-6e0c-45ae-85e8-a2c58e1957c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prod_df['is_consumption'] = 0\n",
    "cons_df['is_consumption'] = 1\n",
    "\n",
    "prod_cons_concat = pd.concat([prod_df, cons_df])\n",
    "\n",
    "train_df_clean = train_df.copy()\n",
    "train_df_clean\n",
    "\n",
    "train_df_clean = train_df_clean.merge(prod_cons_concat, on = ['datetime', 'prediction_unit_id', 'is_consumption'], \n",
    "                     how = 'inner', suffixes = ('_to_remove', ''))\n",
    "train_df_clean.drop(columns = 'target_to_remove', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa89507-e0a9-4d1c-bc2a-e2bc970d9b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_clean.info(show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c7915e-baea-491a-9613-f8f56da64bca",
   "metadata": {},
   "source": [
    "After the imputation, we restore the original ordering of the dataframe using `row_id`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9de763-4f84-407a-9384-e3250085c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_clean.sort_values(by = 'row_id', inplace = True)\n",
    "train_df_clean.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288dbff3-de58-4db7-b3c4-6c46554f51f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99834de3-5fdc-44ac-9d04-96be92058c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_clean.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9881d871-cee0-4044-9819-1ca24c49c342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
